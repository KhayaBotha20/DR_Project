{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c49a154-4be3-4f62-831d-2a86d927d955",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63e5a9-675e-4eb1-977d-2d878b25b39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839387ca-81fc-4ebd-b270-f2ad4af237e2",
   "metadata": {},
   "source": [
    "Verify GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832ecf5-9595-40fd-9a71-dadbaa7c8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bb991-2498-4783-80b1-f74058dfdfa8",
   "metadata": {},
   "source": [
    "Enable mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128156c-c5a1-4121-b418-3a13c91557d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a83e7-fe23-44b7-8c03-417bbf4b3137",
   "metadata": {},
   "source": [
    "Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec226-6502-4ce3-abcb-03f0d20b647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df3951-b2fe-4bad-8f5a-af438527ef91",
   "metadata": {},
   "source": [
    "Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3429930-32f3-4e35-bfe9-5c59b4bb5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class labels to folder names\n",
    "label_to_folder = {\n",
    "    0: 'No_Dr',\n",
    "    1: 'Mild',\n",
    "    2: 'Moderate',\n",
    "    3: 'Severe',\n",
    "    4: 'Proliferate'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007a85e-06cc-453c-983e-f57f7056894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "# Construct full paths\n",
    "base_dir = os.path.join(home_dir, \"Desktop\", \"4th Year Project\", \"Detection Of Diabetic Retinopathy Using Machine Learning.v1i.multiclass\", \"archive\")\n",
    "img_dir = os.path.join(base_dir, \"gaussian_filtered_images\", \"gaussian_filtered_images\")\n",
    "csv_file_path = os.path.join(base_dir, \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76c48d-3402-4360-ae43-96835ae560ee",
   "metadata": {},
   "source": [
    "Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda7e0e-c8e7-4e4e-9b78-c99b8c430bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and Check Paths\n",
    "print(\"Image Directory Path:\", img_dir)\n",
    "print(\"CSV file Path:\", csv_file_path)\n",
    "print(\"Image Directory Exists:\", os.path.exists(img_dir))\n",
    "print(\"CSV file Exists:\", os.path.exists(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d3a02-cbd4-4dd9-ba2e-afcc478fdf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(img_dir) and os.path.exists(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(\"CSV columns:\", df.columns.tolist())\n",
    "    print(df.head())\n",
    "\n",
    "    # Column names in your CSV\n",
    "    id_column = 'id_code'\n",
    "    label_column = 'diagnosis'\n",
    "\n",
    "    # Create full file paths and get labels\n",
    "    file_paths = [os.path.join(img_dir, label_to_folder[label], f\"{id_code}.png\")\n",
    "                  for id_code, label in zip(df[id_column], df[label_column])]\n",
    "    labels = df[label_column].values\n",
    "\n",
    "    # Verify if all files exist\n",
    "    existing_files = [file for file in file_paths if os.path.exists(file)]\n",
    "    existing_labels = [labels[i] for i, file in enumerate(file_paths) if os.path.exists(file)]\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "        existing_files, existing_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define image dimensions\n",
    "    img_width, img_height = 224, 224\n",
    "\n",
    "    # Function to process images\n",
    "    def process_path(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_width, img_height])\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)  # Normalize to [0,1]\n",
    "        return img, label\n",
    "\n",
    "    # Data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomTranslation(0.1, 0.1)\n",
    "    ])\n",
    "\n",
    "    # Create datasets\n",
    "    batch_size = 32\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "    train_dataset = train_dataset.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_files))\n",
    "    train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
    "    val_dataset = val_dataset.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Datasets Have Been Created Successfully!!!\")\n",
    "\n",
    "    # Model definition\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = True  # Unfreeze the base model\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 100\n",
    "\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')  # Ensures output is not in mixed precision\n",
    "    ])\n",
    "\n",
    "    # Model compilation\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=[checkpoint, reduce_lr, early_stopping]\n",
    "    )\n",
    "\n",
    "    # Save the final model\n",
    "    model.save('final_model.keras')\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(val_dataset)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: One or both paths do not exist.\")\n",
    "\n",
    "    # Print the contents of the base directory to help troubleshoot\n",
    "    print(\"\\nContents of base directory:\")\n",
    "    try:\n",
    "        for item in os.listdir(base_dir):\n",
    "            print(item)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base directory not found: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b04832-8d2f-4fc9-b9c2-4d7dce08d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
